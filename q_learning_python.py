# -*- coding: utf-8 -*-
"""Q_Learning_Tahiri_Benaboud_Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OL4Wy2E0IfBPIuKRggpMx_G7m0hQxaG5
"""

#Importation des bibliothèques
import numpy as np
import math
from tabulate import tabulate

"""**Q matrix**"""

#Définition de la forme de l'environnement (ses positions)
environment_rows = 10
environment_columns = 10
#Création un tableau numpy 3D pour contenir la matrice Q actuelle pour chaque paire state/action : Q(s, a) 
#Le tableau contient 10 lignes et 10 colonnes (pour correspondre à l'environnement), ainsi qu'une troisième dimension "action".
#La valeur de chaque paire Q(état, action) est initialisée à 0.
q_values = np.zeros((environment_rows, environment_columns, 4))
print(q_values)

#Code des actions: 0 = Haut, 1 = Droit, 2 = Bas, 3 = Gauche
actions = ['H', 'D', 'B', 'G']

"""**Reward matrix**"""

# Créeation d'un tableau numpy 2D pour contenir les récompenses pour chaque state 
# Le tableau contient 10 lignes et 10 colonnes
rewards = np.full((environment_rows, environment_columns), 1.)

# La récompense du Goal est fixée à 10 pour encourager le robot à l'atteindre
rewards[9, 9] = 10.

# Calcul des récompenses pour tous les autres états en fonction de la distance au Goal.
for x in range(environment_rows):
    for y in range(environment_columns):
        # Sauter le state Goal (il a déjà été défini).
        if (x, y) == (9, 9):
            continue
        
        # Calcul de la distance entre l'état actuel et le goal.
        distance = math.sqrt((x - 9)**2 + (y - 9)**2)
        
        # On met la récompense pour l'état actuel à l'inverse de la distance.
        rewards[x, y] = 1.0 / distance

#Définir une récompense négative pour les obstacles à -10 pour punir le robot.
rewards[0,5] = -10
rewards[0,6] = -10
rewards[3,3] = -10
rewards[3,4] = -10
rewards[6,5] = -10
rewards[6,6] = -10
# Affichage de la matrice de récompenses
rounded_rewards = np.round(rewards, 3)
print(tabulate(rounded_rewards, tablefmt='grid'))

"""**Fonctions d'aide**"""

# Fonction pour vérifier si l'état actuel est un état terminal aka (goal)
def is_terminal_state(actual_x, actual_y):
  # Si la récompense pour cet état est différente de 10, alors il ne s'agit pas d'un état terminal. 
  if rewards[actual_x, actual_y] != 10:
    return False
  else:
    return True
# Fonction pour choisir un état de départ aléatoire pour boucler dans l'environnement
def get_random_starting_location():
  actual_x = np.random.randint(environment_rows)
  actual_y = np.random.randint(environment_columns)
  # Continuer à choisir des index de lignes et de colonnes aléatoires même lorsque le goal est atteint
  while is_terminal_state(actual_x, actual_y):
    actual_x = np.random.randint(environment_rows)
    actual_y = np.random.randint(environment_columns)
  return actual_x, actual_y
  
#un algorithme avide d'epsilon qui choisira l'action suivante (c'est-à-dire l'endroit où se déplacer ensuite)
def choose_next_action(actual_x, actual_y, epsilon):
  # Si une valeur choisie au hasard entre 0 et 1 est inférieure à epsilon
  # on choisit alors la valeur la plus prometteuse de la matrice Q pour cet état.
  if np.random.random() < epsilon:
    return np.argmax(q_values[actual_x, actual_y])
  # Sinon on choisit une action aléatoire
  else: 
    return np.random.randint(4)

# Fonction qui obtiendra la prochaine position "état suivant" en fonction de l'action choisie.
def get_next_state(actual_x, actual_y, action_index):
  next_x = actual_x
  next_y = actual_y
  if actions[action_index] == 'H' and actual_x > 0:
    next_x -= 1
  elif actions[action_index] == 'D' and actual_y < environment_columns - 1:
    next_y += 1
  elif actions[action_index] == 'B' and actual_x < environment_rows - 1:
    next_x += 1
  elif actions[action_index] == 'G' and actual_y > 0:
    next_y -= 1
  return next_x, next_y

# Fonction qui trouvera le chemin optimale entre n'importe quel état initial et l'état finale
def shortest_path(start_x, start_y):
  # Retourner immédiatement si l'état initiale est l'état finale
  if is_terminal_state(start_x, start_y):
    return []
  else:
    actual_x, actual_y = start_x, start_y
    shortest_path = []
    shortest_path.append([actual_x, actual_y])
    # Se déplacer sans arrêt le long du chemin jusqu'à ce que nous atteignions le goal
    while not is_terminal_state(actual_x, actual_y):
      # On obtient l'action optimale à prendre
      action_index = choose_next_action(actual_x, actual_y,1)
      # Se déplacer vers l'état suivant suivant sur le chemin, et ajouter le nouvel état à la liste
      next_x, next_y = get_next_state(actual_x, actual_y, action_index)
      if (next_x,next_y != actual_x,actual_y):
        actual_x,actual_y = next_x,next_y
        shortest_path.append([actual_x, actual_y])
      else:
        continue
    return shortest_path

"""**Learning episodes**"""

# Le pourcentage de temps où nous devrions prendre l'action optimale (au lieu d'une action aléatoire)
epsilon = 0.9 
# Paramètre d'apprentissage gamma
gamma = 0.7

# Exécuter 1000 épisodes d'entrainement
for episode in range(1000):
  # On choisit un état initiale aléatoire
  x, y = get_random_starting_location()
  # On explore notre environment jusqu'à ce qu'on arrive à l'état finale
  while not is_terminal_state(x, y):
    # On choisit une action pour se déplacer
    action_index = choose_next_action(x, y, epsilon)
    # On se déplace vers le prochain état à l'aide de l'action choisie
    past_x, past_y = x, y # on stocke l'ancien état
    x, y = get_next_state(x, y, action_index)
    # On reçoit la récompense du passage à l'état suivant
    # On calcule les nouvelles valeurs de la matrice Q pour l'ancien état
    reward = rewards[x, y]
    new_q_value = reward + (gamma * np.max(q_values[x, y]))
    # On met à jour la valeur Q pour l'ancien état et l'action performée
    q_values[past_x, past_y, action_index] = new_q_value

# Après l'exécution de 1000 épisodes, on affiche un message
print('Exploration terminée !')
# Affichage de la matrice Q
print(q_values)

"""**Chemin optimale**"""

# Afficher quelques chemins optimales
print(shortest_path(8, 9))
print(shortest_path(0, 0))